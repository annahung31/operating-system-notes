# 各種 RAID

# RAID 0 & RAID 1

## RAID 0： 只用 striping

藉由平行讀取增加 performance

I/O bandwidth 正比於硬碟數量。

![%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.33.36.png](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.33.36.png)

![%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.33.42.png](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.33.42.png)

A 是同一個 data block，切割後放在不同硬碟。

## RAID 1： 只做 Mirrored

備份：增加 reliability 

![%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.35.04.png](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.35.04.png)

一般來說備份三份就夠了。

但是四顆硬碟只能儲存一顆的資料量（另外三顆用來備份），儲存率低。 

讀的 bandwidth：可以變 N 倍，因為各顆硬碟可以從不同地方開始讀。

寫的 bandwidth：一樣，甚至差一點。

假設一份資料需要用額外一倍的容量做備份， overhead = 100%

overhead = $額外容量/資料容量$ 

---

包含 Error Collection: RAID 2~4

# RAID 2:Hamming code

![%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.35.34.png](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.35.34.png)

利用 coding 解決 data 使用率低的狀況。找出哪裡壞掉？去修正它

D: data

B: bit

可以算出錯的是誰，知道它原來是 1 還是 0，將他還原回來。

錯 １個，可以知道，而且可以修復

錯 ２ 個，可以知道，無法修復

需要 4 個 data + 3 個 [parity bits](https://www.notion.so/parity-bit-check-bit-a52940a9b148425ba28c5bedc9171d7a) 才能夠做這件事，各位在一個硬碟上，所以共需要有 7 顆硬碟。

相較於 RAID 1 ，比較有效率地利用空間。

overhead =  $額外容量/資料容量$  = 3/4 = 75%

 

[overhead：浪費](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/overhead%EF%BC%9A%E6%B5%AA%E8%B2%BB%20f3e577d9ab404c269355798f62cbd4f5.md)

[[註] parity bit (check bit)](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/%5B%E8%A8%BB%5D%20parity%20bit%20(check%20bit)%20a52940a9b148425ba28c5bedc9171d7a.md)

# RAID 3 & RAID 4

![%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.36.17.png](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.36.17.png)

但其實每個 sector 本來就具備 ECC，所以其實不需要做 detection，只要做 recovery 就好了。所以只要用 1 個 parity bit 來還原壞掉的地方是什麼值。

上圖例子的 overhead = 1/3 = 33%

硬碟越多顆， overhead 越低，但是壞掉的機率越高，越不保險。

### 3 和 4 的差異

RAID 3: by Bit，需要做同步化，才能讀取一個東西，比較慢。

RAID 4: by block ，只要讀一個東西，就可以得到所需的資料，不需要同步化。(has higher I/O throughput)

使用者不需要讀 parity bit，所以對使用者來說， disk 3 等於沒有作用。 bandwidth 只有 N-1

寫的時候，存 parity bit 的硬碟是 bottle neck，因為所有人都要想著寫它。

# RAID 5: Distributed Parity

![%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.36.49.png](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.36.49.png)

parity bit 的位置不同。 散落在每個 Disk 上，

讀： bandwidth = N 倍

寫：

1. 法一：把資料都讀出來，算出 parity ，再存回去。 

    花費了 N -2 個 read （ 2  是因為 modified bit 跟 parity bit 可以不用 read），再寫 2 個 write ，總共  $(N-2)+2$ 個 request。

    N 顆硬碟 / $(N-2)+2$ 個 request  = 1 ，效率還是跟只有一顆硬碟一樣。

2. 法二：只要把修改的人，跟 parity bit 讀出來，計算差值，再把 parity bit 寫回去。

    不管有幾顆硬碟，request 都是 2 個 read + 2 個 write = 4

    N 顆硬碟 / 4 = N/4 time faster。

![%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-14_10.47.36.png](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-14_10.47.36.png)

# RAID 6:P+Q Dual Parity Redundancy

![%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.37.31.png](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.37.31.png)

更多的 parity bit，解決多個硬碟壞掉的重建

實際上應用比較少用了，因為浪費比較多的空間，讀寫的 performance 也不見得比較好。

用在資料真的很重要的時候

# Hybrid RAID

![%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.37.51.png](%E5%90%84%E7%A8%AE%20RAID%20a368389b46454b9aaadaa7f168c4064e/_2020-06-13_12.37.51.png)

不同 RAID 的結合。

RAID 0+1 = 先做 striping 再做 Mirrored

RAID 1 +0 = 先做 Mirrored 再做  striping。讀出來壞掉的機率比較小，delay 比較少。因為在 local RAID 1 時有 controller 會先判斷有沒有成功讀出來，萬一壞掉，可以改讀備份資料。

# 薛 QA

共有兩顆硬碟，壞掉的機率是 0.5，每次修理要 10 小時，lifetime 是 100000小時，因此平均可以修理 100000/10 次，修理完再撐 100000小時，兩顆同時壞掉前可以再撐：

0.5 * (100000/10) *  100000 

# Reference

[http://ocw.nthu.edu.tw/ocw/upload/141/news/周志遠教授作業系統_chap12＿Operating System Chap12 Mass Storage System ＿.pdf](http://ocw.nthu.edu.tw/ocw/upload/141/news/%E5%91%A8%E5%BF%97%E9%81%A0%E6%95%99%E6%8E%88%E4%BD%9C%E6%A5%AD%E7%B3%BB%E7%B5%B1_chap12%EF%BC%BFOperating%20System%20Chap12%20Mass%20Storage%20System%20%EF%BC%BF.pdf)